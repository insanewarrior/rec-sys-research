{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_num_threads(16)\n",
    "torch.set_num_interop_threads(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cornac\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.models import MF, PMF, BPR, SANSA, BiVAECF, LightGCN, RecVAE, EASE, NGCF, VAECF, IBPR, NeuMF, HPF\n",
    "from cornac.metrics import Precision, Recall, NDCG, MAP, MRR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_ranking_split(\n",
    "    df: pd.DataFrame,\n",
    "    entity_field: str,\n",
    "    test_size: float = 0.1,\n",
    "    random_state: int | None = None,\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits a ranking-based dataset into training and test sets while preserving the entity distribution.\n",
    "\n",
    "    This function is useful for ranking models (e.g., next-best-offer, personalized recommendations) where each\n",
    "    entity (e.g., user) has multiple interactions with different items, and stratification ensures that\n",
    "    different user interaction levels are maintained in both splits.\n",
    "\n",
    "    Parameters:\n",
    "        df: The ranking dataset, where each row represents an interaction between an entity (e.g., user)\n",
    "            and an item (e.g., game, offer).\n",
    "        entity_field: The column representing the entity to be stratified.\n",
    "        test_size: Fraction of unique entities to allocate to the test set.\n",
    "        random_state: Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        DataFrames containing training and test data.\n",
    "\n",
    "    Example:\n",
    "        >>> train_validation_df, test_df = stratified_ranking_split(df, entity_field='user_id', test_size=0.1)\n",
    "        >>> train_df, validation_df = stratified_ranking_split(\n",
    "        ...     train_validation_df, entity_field='user_id', test_size=0.1\n",
    "        ... )\n",
    "        >>> print(train_df.shape, validation_df.shape, test_df.shape)\n",
    "    \"\"\"\n",
    "    entity_interaction_counts = df[entity_field].value_counts()\n",
    "\n",
    "    interaction_frequencies = entity_interaction_counts.value_counts()\n",
    "    stratifiable_interaction_counts = interaction_frequencies[interaction_frequencies >= 2].index\n",
    "\n",
    "    stratifiable_entities = entity_interaction_counts[\n",
    "        entity_interaction_counts.isin(stratifiable_interaction_counts)\n",
    "    ].index\n",
    "    non_stratifiable_entities = entity_interaction_counts[\n",
    "        ~entity_interaction_counts.isin(stratifiable_interaction_counts)\n",
    "    ].index\n",
    "\n",
    "    train_strat, test_strat = (\n",
    "        train_test_split(\n",
    "            stratifiable_entities,\n",
    "            test_size=test_size,\n",
    "            stratify=entity_interaction_counts[stratifiable_entities],\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        if len(stratifiable_entities) > 1\n",
    "        else (stratifiable_entities, [])\n",
    "    )\n",
    "\n",
    "    if len(non_stratifiable_entities) > 1:\n",
    "        train_non_strat, test_non_strat = train_test_split(\n",
    "            non_stratifiable_entities,\n",
    "            test_size=test_size,\n",
    "            random_state=random_state,\n",
    "        )\n",
    "    else:\n",
    "        train_non_strat = non_stratifiable_entities\n",
    "        test_non_strat = []\n",
    "\n",
    "    train_users = np.concatenate([train_strat, train_non_strat])\n",
    "    test_users = np.concatenate([test_strat, test_non_strat])\n",
    "\n",
    "    return df[df[entity_field].isin(train_users)], df[df[entity_field].isin(test_users)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123\n",
    "\n",
    "metrics = [Precision(k=10), Recall(k=10), NDCG(k=10), MAP(), MRR()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastfm_dataset = (\n",
    "    pd.read_csv(\n",
    "        \"/Users/a-shyraliev/phd/rec-sys-research/collab_filtering_battlefield/lastfm-dataset-360K/usersha1-artmbid-artname-plays.tsv\",\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        usecols=[0, 2, 3],\n",
    "        names=['user_id', 'item_id', 'play_count'],\n",
    "    )\n",
    "    .loc[:, ['user_id', 'item_id', 'play_count']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358868, 292363)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastfm_dataset['user_id'].nunique(), lastfm_dataset['item_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, lastfm_dataset_sample_str = stratified_ranking_split(\n",
    "    lastfm_dataset,\n",
    "    entity_field='user_id',\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "del lastfm_dataset, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a-shyraliev/.pyenv/versions/3.12.2/lib/python3.12/site-packages/cornac/data/dataset.py:335: UserWarning: 5 duplicated observations are removed!\n",
      "  warnings.warn(\"%d duplicated observations are removed!\" % dup_count)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<35887x93612 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1402848 stored elements in Compressed Sparse Row format>,\n",
       " <35887x93612 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 339585 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs = RatioSplit(data=lastfm_dataset_sample_str.values, test_size=0.2, rating_threshold=0.0, seed=SEED)\n",
    "rs.train_set.csr_matrix, rs.test_set.csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del lastfm_dataset_sample_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MF hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_models = [\n",
    "    MF(use_bias=True, seed=SEED),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "   | MAP |    MRR | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "-- + --- + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "MF | nan | 0.0003 |  0.0000 |       0.0000 |    0.0000 |    0.4127 | 143.9471\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=mf_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. PMF hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmf_models = [\n",
    "    PMF(seed=SEED),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "    |    MAP |    MRR | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "--- + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "PMF | 0.0001 | 0.0001 |  0.0000 |       0.0000 |    0.0000 |   12.6263 | 338.8230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=pmf_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. BPR hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpr_models = [\n",
    "    BPR(seed=SEED),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "    |    MAP |    MRR | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "--- + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "BPR | 0.0273 | 0.1294 |  0.0424 |       0.0343 |    0.0358 |   11.6591 | 451.1734\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=bpr_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. BiVAECF hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bivaecf_models = [\n",
    "    BiVAECF(use_gpu=False, seed=SEED),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "        |    MAP |    MRR | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "------- + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "BiVAECF | 0.0637 | 0.2451 |  0.0930 |       0.0757 |    0.0796 | 7616.4112 | 398.0724\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=bivaecf_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. RecVAE hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "recvae_models = [\n",
    "    RecVAE(n_epochs=10, use_gpu=False, seed=SEED),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "       |    MAP |    MRR | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "------ + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "RecVae | 0.0763 | 0.2847 |  0.1142 |       0.0925 |    0.0979 | 6903.3554 | 813.5656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=recvae_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. EASE hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ease_models = [\n",
    "    EASE(seed=SEED),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=ease_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93,612 × 93,612=8,763,220,944 elements \n",
    "\n",
    "Assuming each float is 8 bytes (float64), that matrix alone would use:\n",
    "8,763,220,944 × 8≈70.1 GB8\n",
    "\n",
    "That’s just one matrix — and there are intermediate copies made during matrix operations, inversion, etc., \n",
    "so actual memory usage can easily balloon to 100–150 GB RAM or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. HPF hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpf_models = [\n",
    "    HPF(seed=SEED),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning...\n",
      "Learning completed!\n",
      "\n",
      "TEST:\n",
      "...\n",
      "    |    MAP |    MRR | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "--- + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "HPF | 0.0422 | 0.1825 |  0.0641 |       0.0517 |    0.0538 |  370.1086 | 386.1145\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=hpf_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. IBPR hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibpr_models = [\n",
    "    IBPR(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=ibpr_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## too long to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. SANSA hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sansa_models = [\n",
    "    SANSA(verbose=False, seed=SEED),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sansa.core.factorizers:Computing incomplete Cholesky decomposition of X^TX + 1.0*I...\n",
      "INFO:sansa.core.factorizers:Finding a fill-in reducing ordering (method = colamd)...\n",
      "INFO:sansa.core.factorizers:Computing X^TX...\n",
      "INFO:sansa.core.factorizers:\n",
      "                X^TX info:\n",
      "                    shape = (93612, 93612) \n",
      "                    nnz = 26274852 \n",
      "                    density = 0.299831% \n",
      "                    size = 210.6 MB\n",
      "                \n",
      "INFO:sansa.core.factorizers:Sorting indices of A...\n",
      "INFO:sansa.core.factorizers:Casting indptr of A to int64...\n",
      "INFO:sansa.core.factorizers:Casting indices of A to int64...\n",
      "INFO:sansa.core.factorizers:Computing approximate Cholesky decomposition (method = ICF)...\n",
      "INFO:sansa.core.factorizers:Scaling columns and creating diagonal matrix D (LL^T -> L'DL'^T)...\n",
      "INFO:sansa.core.inverters:Calculating initial guess using 1 step of Schultz method...\n",
      "INFO:sansa.core.inverters:Calculating approximate inverse using Uniform Minimal Residual algorithm...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.7384495735168457, relative Frobenius norm squared: 0.0034407598432153463\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR scan 1...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.7384495735168457, relative Frobenius norm squared: 0.0034407598432153463\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR scan 2...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.30587220191955566, relative Frobenius norm squared: 0.002833602949976921\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR scan 3...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.05642279237508774, relative Frobenius norm squared: 0.00020451104501262307\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR finetune step 1...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.05089490860700607, relative Frobenius norm squared: 0.00020405171380843967\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR finetune step 2...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.05033910647034645, relative Frobenius norm squared: 0.0002033205673797056\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR finetune step 3...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.05033910647034645, relative Frobenius norm squared: 0.0002026166912401095\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR finetune step 4...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.05033910647034645, relative Frobenius norm squared: 0.0002020141255343333\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR finetune step 5...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.05033910647034645, relative Frobenius norm squared: 0.00020147187751717865\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR finetune step 6...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.05033910647034645, relative Frobenius norm squared: 0.00020095818035770208\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR finetune step 7...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.05033910647034645, relative Frobenius norm squared: 0.0002004981943173334\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR finetune step 8...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.05033910647034645, relative Frobenius norm squared: 0.00020001937809865922\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR finetune step 9...\n",
      "INFO:sansa.core._ops._inverse_ops:Current maximum residual: 0.05033910647034645, relative Frobenius norm squared: 0.0001995885104406625\n",
      "INFO:sansa.core._ops._inverse_ops:Performing UMR finetune step 10...\n",
      "INFO:sansa.core.inverters:Current maximum residual: 0.05033910647034645, relative Frobenius norm squared: 0.00019916935707442462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "      |    MAP |    MRR | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "----- + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "SANSA | 0.0659 | 0.2583 |  0.1029 |       0.0832 |    0.0880 |  584.8892 | 786.6559\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=sansa_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. VAECF hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaecf_models = [\n",
    "    VAECF(use_gpu=False, seed=SEED),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "      |    MAP |    MRR | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "----- + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "VAECF | 0.0675 | 0.2632 |  0.1019 |       0.0809 |    0.0851 | 3870.0494 | 498.0539\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=vaecf_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. NeuMF hyperparams optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neumf_models = [\n",
    "    NeuMF(verbose=False, backend=\"pytorch\", seed=SEED),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "      |    MAP |    MRR | NDCG@10 | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "----- + ------ + ------ + ------- + ------------ + --------- + --------- + --------\n",
      "NeuMF | 0.0459 | 0.1531 |  0.0567 |       0.0503 |    0.0529 | 1231.0541 | 616.6095\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cornac.Experiment(\n",
    "    eval_method=rs,\n",
    "    models=neumf_models,\n",
    "    metrics=metrics,\n",
    "    user_based=True,\n",
    "    save_dir=None,\n",
    ").run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
